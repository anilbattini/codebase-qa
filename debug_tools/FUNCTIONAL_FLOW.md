# FUNCTIONAL FLOW DIAGRAM

## ðŸŸ¦ RAG Index Build & Ready Flow

Sidebar: user selects project directory, project type, model provider, and endpoint       
â†“  
[UIComponents.render_sidebar_config, Line no: 19, core/ui_components.py](https://github.com/anilbattini/codebase-qa/blob/main/core/ui_components.py#L19)  
  â””â”€ **ENHANCED**: Renders sidebar inputs for project directory, project type, model provider selection, and provider-specific configuration.  
  â””â”€ **NEW**: Model Provider dropdown with Ollama and Hugging Face options.  
  â””â”€ **NEW**: Provider-specific configuration (Ollama endpoint vs Hugging Face cache info).  
  â””â”€ Handles user changes, validations, and warnings about existing data.  
â†“  
[RagManager.initialize_session_state, Line no: 14, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L14)  
  â””â”€ Initializes critical Streamlit session state keys like retriever and QA chain for stable app state.  
  â””â”€ Ensures consistent session during build or load operations.  
â†“  
[ProjectConfig.__init__, Line no: 112, core/config.py](https://github.com/anilbattini/codebase-qa/blob/main/core/config.py#L112)  
  â””â”€ Detects or applies project type (auto/manual).  
  â””â”€ Loads language specific file extensions, chunking rules, ignore patterns, and project indicators.  
â†“  
[ModelConfig._initialize_provider, Line no: 63, core/model_config.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_config.py#L63)  
  â””â”€ **NEW**: Initializes the selected model provider (Ollama or Hugging Face).  
  â””â”€ **NEW**: Sets up provider-specific configuration and availability checks.  
  â””â”€ **NEW**: Handles provider switching with cache management.  
â†“  
[RagManager.should_rebuild_index, Line no: 123, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L123)  
  â””â”€ **NEW**: Returns detailed rebuild information: {"rebuild": bool, "reason": str, "files": list}  
  â””â”€ Checks presence of vector DB SQLite file, git/hash tracking files.  
  â””â”€ **ENHANCED**: Detects if source files have changed via Git commit differences or working directory changes.  
  â””â”€ **NEW**: Distinguishes between incremental rebuild (changed files) and full rebuild (no DB, no tracking).  

**REBUILD DECISION LOGIC:**
- **No Database**: `{"rebuild": True, "reason": "no_database", "files": None}` â†’ Full rebuild
- **No Tracking**: `{"rebuild": True, "reason": "no_tracking", "files": None}` â†’ Full rebuild  
- **Files Changed**: `{"rebuild": True, "reason": "files_changed", "files": [file_list]}` â†’ Incremental rebuild
- **No Changes**: `{"rebuild": False, "reason": "no_changes", "files": []}` â†’ Load existing

**FORCE REBUILD HANDLING:**
If user requests force rebuild (via "ðŸ”„ Force Rebuild" button)  
â†“  
[App.py force rebuild logic, Line no: 54, core/app.py](https://github.com/anilbattini/codebase-qa/blob/main/core/app.py#L54)  
  â””â”€ **NEW**: Clears existing data and performs complete rebuild regardless of file changes.  
  â””â”€ **NEW**: Shows user confirmation: "ðŸ”„ Force Rebuild: User requested complete rebuild. Cleaning existing data..."  

**INCREMENTAL REBUILD (Changed Files Detected):**
â†“  
[RagManager.build_rag_index with incremental=True, Line no: 162, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L162)  
  â””â”€ **NEW**: Calls build_rag with incremental=True and files_to_process parameter.  
  â””â”€ **NEW**: Shows progress: "ðŸ”„ Incremental Build: Processing X changed files..."  
  â””â”€ **NEW**: Preserves existing database and only processes changed files.  

**FULL REBUILD (No DB, No Tracking, or Force Rebuild):**
â†“  
[RagManager.cleanup_existing_files, Line no: 41, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L41)  
  â””â”€ Deletes existing vector DB directories and clears session state to ensure clean rebuild.  
  â””â”€ Handles retry logic to overcome file locks during cleanup.  
â†“  
[RagManager.build_rag_index with incremental=False, Line no: 162, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L162)  
  â””â”€ **NEW**: Calls build_rag with incremental=False for full rebuild.  
  â””â”€ **NEW**: Shows progress: "ðŸ”„ Full Build: Rebuilding entire RAG index..."  

**BUILD PROCESS (build_rag function):**
â†“  
[build_rag, Line no: 62, core/build_rag.py](https://github.com/anilbattini/codebase-qa/blob/main/core/build_rag.py#L62)  
  â””â”€ **NEW**: Supports incremental=True/False and files_to_process parameters.  
  â””â”€ **NEW**: For incremental builds: preserves existing database, processes only changed files.  
  â””â”€ **NEW**: For full builds: cleans database directory, processes all files.  
  â””â”€ **ENHANCED**: Shows build mode: "ðŸ”„ Incremental build mode - processing only changed files" or "Full build mode".  
  â””â”€ Reads files, applies chunking per language, extracts semantic metadata.  
  â””â”€ Creates output folders and handles incremental file processing via git/hash tracking.  
  â””â”€ **ENHANCED**: Initializes embedding model via ModelProvider interface (Ollama or Hugging Face).  
  â””â”€ **NEW**: Uses in-memory caching to prevent model reloading.  
  â””â”€ Processes each source file: chunks files, extracts metadata, deduplicates using chunk fingerprints.  
  â””â”€ Builds code relationships and hierarchical index files.  
  â””â”€ Sanitizes chunks and sends batches for embedding via selected provider API.  
  â””â”€ **ENHANCED**: Persists embeddings in Chroma vector database with incremental update support.  
  â””â”€ Updates file tracking and logs detailed stats on the process.  
â†“  
[ProjectConfig.create_directories, Line no: 181, core/config.py](https://github.com/anilbattini/codebase-qa/blob/main/core/config.py#L181)  
  â””â”€ Ensures all database and logs directories exist for current project type.  
â†“  
[FileHashTracker.get_changed_files, core/git_hash_tracker.py](https://github.com/anilbattini/codebase-qa/blob/main/core/git_hash_tracker.py)  
  â””â”€ **ENHANCED**: Now properly detects Git commit differences using `git diff --name-only {last_commit}..{current_commit}`.  
  â””â”€ **NEW**: Combines commit differences + working directory changes for comprehensive change detection.  
  â””â”€ **NEW**: Provides detailed logging: "Git commit diff detected X changed files between {last_commit} and {current_commit}".  
  â””â”€ **NEW**: Falls back to content-hash tracking if Git tracking fails.  
â†“  
[ModelProvider.get_embedding_model, Line no: 63, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py#L63)  
  â””â”€ **NEW**: Retrieves embedding model via abstract ModelProvider interface.  
  â””â”€ **NEW**: Supports both Ollama and Hugging Face providers.  
  â””â”€ **NEW**: Implements in-memory caching to prevent model reloading.  
  â””â”€ **NEW**: Uses custom cache directory for Hugging Face models.  
â†“  
[ModelConfig.get_current_provider, core/model_config.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_config.py)  
  â””â”€ **NEW**: Returns the current model provider instance (Ollama or Hugging Face).  
  â””â”€ **NEW**: Handles provider switching with cache management.  
â†“  
[chunker_factory.get_chunker, core/chunker_factory.py](https://github.com/anilbattini/codebase-qa/blob/main/core/chunker_factory.py)  
  â””â”€ Selects language-specific chunking strategy function per file extension.  
â†“  
[chunker_factory.chunker, core/chunker_factory.py](https://github.com/anilbattini/codebase-qa/blob/main/core/chunker_factory.py)  
  â””â”€ Executes semantic chunking, splitting code into meaningful segments for embedding.  
â†“  
[MetadataExtractor.create_enhanced_metadata, core/metadata_extractor.py](https://github.com/anilbattini/codebase-qa/blob/main/core/metadata_extractor.py)  
  â””â”€ Extracts detailed chunk meta class names, function names, dependencies, semantic anchors.  
â†“  
[chunk_fingerprint, Line no: 13, core/build_rag.py](https://github.com/anilbattini/codebase-qa/blob/main/core/build_rag.py#L13)  
  â””â”€ Generates SHA-256 hash fingerprints for each chunk to eliminate duplicate embeddings.  
â†“  
[chunker_factory.summarize_chunk, core/chunker_factory.py](https://github.com/anilbattini/codebase-qa/blob/main/core/chunker_factory.py)  
  â””â”€ Creates concise summary keywords from chunks for retrieval relevance boosts.  
â†“  
[ModelProvider.embed_documents, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py)  
  â””â”€ **NEW**: Embeds chunks using the selected provider (Ollama or Hugging Face).  
  â””â”€ **NEW**: Uses cached model instances to prevent reloading.  
  â””â”€ **NEW**: Handles provider-specific embedding formats and error handling.  
â†“  
[Chroma.persist, core/build_rag.py](https://github.com/anilbattini/codebase-qa/blob/main/core/build_rag.py)  
  â””â”€ **ENHANCED**: Persists embeddings in Chroma vector database with incremental update support.  
  â””â”€ **NEW**: Handles both full rebuild and incremental update scenarios.  
â†“  
[FileHashTracker.update_tracking, core/git_hash_tracker.py](https://github.com/anilbattini/codebase-qa/blob/main/core/git_hash_tracker.py)  
  â””â”€ **ENHANCED**: Updates file tracking with new commit SHA and file hashes.  
  â””â”€ **NEW**: Supports incremental tracking for changed files only.  
â†“  
[ProjectConfig.create_directories, Line no: 181, core/config.py](https://github.com/anilbattini/codebase-qa/blob/main/core/config.py#L181)  
  â””â”€ Ensures all database and logs directories exist for current project type.  
â†“  
[RagManager.build_rag_index, Line no: 162, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L162)  
  â””â”€ **ENHANCED**: Stores retriever in session state for later QA chain setup.  
  â””â”€ **NEW**: Supports both incremental and full rebuild modes.  
  â””â”€ **NEW**: Logs detailed build information and performance metrics.  
â†“  
[App.py success message, Line no: 150, core/app.py](https://github.com/anilbattini/codebase-qa/blob/main/core/app.py#L150)  
  â””â”€ **ENHANCED**: Shows appropriate success message based on build mode.  
  â””â”€ **NEW**: "âœ… RAG index built successfully!" for full builds.  
  â””â”€ **NEW**: "âœ… RAG index rebuilt successfully!" for incremental builds.  

**RAG INDEX READY:**
â†“  
[App.py chat interface, Line no: 200, core/app.py](https://github.com/anilbattini/codebase-qa/blob/main/core/app.py#L200)  
  â””â”€ **ENHANCED**: Shows chat interface only when RAG is ready.  
  â””â”€ **NEW**: Renders chat history with metadata display.  
  â””â”€ **NEW**: Sets up chat handler with proper error handling.  
  â””â”€ **NEW**: Processes queries using the selected model provider.  

## ðŸŸ© Chat Query Processing Flow

User enters query in chat input form  
â†“  
[UIComponents.render_chat_input, Line no: 500, core/ui_components.py](https://github.com/anilbattini/codebase-qa/blob/main/core/ui_components.py#L500)  
  â””â”€ **ENHANCED**: Renders chat input form with proper form submission handling.  
  â””â”€ **NEW**: Always enabled when RAG is ready (no more disabled state).  
â†“  
[ChatHandler.process_query, Line no: 43, core/chat_handler.py](https://github.com/anilbattini/codebase-qa/blob/main/core/chat_handler.py#L43)  
  â””â”€ **ENHANCED**: Enhanced query processing with intent classification, rewriting, and RAG support.  
  â””â”€ **NEW**: Supports both RAG-enabled and RAG-disabled modes.  
  â””â”€ **NEW**: Uses in-memory cached models for faster processing.  
â†“  
[QueryIntentClassifier.classify_intent, Line no: 36, core/query_intent_classifier.py](https://github.com/anilbattini/codebase-qa/blob/main/core/query_intent_classifier.py#L36)  
  â””â”€ **ENHANCED**: Classifies query intent with confidence scoring.  
  â””â”€ **NEW**: Provides query context hints for better retrieval.  
â†“  
[ChatHandler._rewrite_query_with_intent, Line no: 300, core/chat_handler.py](https://github.com/anilbattini/codebase-qa/blob/main/core/chat_handler.py#L43)  
  â””â”€ **ENHANCED**: Rewrites queries based on intent for better retrieval.  
  â””â”€ **NEW**: Uses LLM chain for intelligent query rewriting.  
â†“  
[RagManager.get_retriever, Line no: 380, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L380)  
  â””â”€ **NEW**: Lazy loading of retriever - only loads when needed.  
  â””â”€ **NEW**: Prevents unnecessary model loading during RAG loading.  
â†“  
[Retriever.invoke, core/chat_handler.py](https://github.com/anilbattini/codebase-qa/blob/main/core/chat_handler.py#L43)  
  â””â”€ **ENHANCED**: Retrieves relevant documents using rewritten query.  
  â””â”€ **NEW**: Falls back to original query if no results.  
  â””â”€ **NEW**: Tries key terms if still no results.  
â†“  
[ContextBuilder.build_enhanced_context, core/context_builder.py](https://github.com/anilbattini/codebase-qa/blob/main/core/context_builder.py)  
  â””â”€ **ENHANCED**: Builds enhanced context window using retrieved documents.  
  â””â”€ **NEW**: Incorporates intent-aware context building.  
â†“  
[RagManager.lazy_get_qa_chain, Line no: 350, core/rag_manager.py](https://github.com/anilbattini/codebase-qa/blob/main/core/rag_manager.py#L350)  
  â””â”€ **NEW**: Lazy loading of QA chain - only created when needed.  
  â””â”€ **NEW**: Uses cached retriever and loads LLM model on demand.  
  â””â”€ **NEW**: Implements in-memory caching for QA chain.  
â†“  
[ModelProvider.get_llm_model, Line no: 300, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py#L300)  
  â””â”€ **NEW**: Retrieves LLM model via abstract ModelProvider interface.  
  â””â”€ **NEW**: Supports both Ollama and Hugging Face providers.  
  â””â”€ **NEW**: Implements in-memory caching to prevent model reloading.  
â†“  
[QAChain.invoke, core/chat_handler.py](https://github.com/anilbattini/codebase-qa/blob/main/core/chat_handler.py#L43)  
  â””â”€ **ENHANCED**: Generates answer using enhanced context and query.  
  â””â”€ **NEW**: Falls back to direct LLM call if QA chain fails.  
  â””â”€ **NEW**: Handles provider-specific response formats.  
â†“  
[App.py chat history update, Line no: 220, core/app.py](https://github.com/anilbattini/codebase-qa/blob/main/core/app.py#L220)  
  â””â”€ **NEW**: Stores chat history with metadata in session state.  
  â””â”€ **NEW**: Forces UI refresh to show new chat history.  
  â””â”€ **NEW**: Handles errors gracefully with user feedback.  

## ðŸŸ¨ Model Provider Management Flow

User selects model provider in sidebar  
â†“  
[UIComponents.render_sidebar_config, Line no: 76, core/ui_components.py](https://github.com/anilbattini/codebase-qa/blob/main/core/ui_components.py#L76)  
  â””â”€ **NEW**: Model Provider dropdown with Ollama and Hugging Face options.  
  â””â”€ **NEW**: Provider-specific configuration display.  
â†“  
[ModelConfig.switch_to_ollama/switch_to_huggingface, core/model_config.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_config.py)  
  â””â”€ **NEW**: Handles provider switching with proper initialization.  
  â””â”€ **NEW**: Sets provider type and initializes provider instance.  
â†“  
[set_provider, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py)  
  â””â”€ **NEW**: Global provider management function.  
  â””â”€ **NEW**: Clears previous provider cache to prevent memory issues.  
  â””â”€ **NEW**: Creates new provider instance via factory pattern.  
â†“  
[ModelProviderFactory.create_provider, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py)  
  â””â”€ **NEW**: Factory pattern for creating provider instances.  
  â””â”€ **NEW**: Supports both Ollama and Hugging Face providers.  
â†“  
[ModelProvider.check_availability, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py)  
  â””â”€ **NEW**: Provider-specific availability checks.  
  â””â”€ **NEW**: Ollama: HTTP endpoint responsiveness.  
  â””â”€ **NEW**: Hugging Face: Local model availability and cache status.  
â†“  
[ModelProvider initialization, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py)  
  â””â”€ **NEW**: Provider-specific initialization.  
  â””â”€ **NEW**: Ollama: Sets endpoint URL.  
  â””â”€ **NEW**: Hugging Face: Creates custom cache directory and sets environment variables.  
â†“  
[ModelProvider.get_embedding_model/get_llm_model, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py)  
  â””â”€ **NEW**: Provider-specific model loading with in-memory caching.  
  â””â”€ **NEW**: Ollama: Uses langchain_ollama for API calls.  
  â””â”€ **NEW**: Hugging Face: Downloads and caches local models.  
â†“  
[Model caching and reuse, core/model_provider.py](https://github.com/anilbattini/codebase-qa/blob/main/core/model_provider.py)  
  â””â”€ **NEW**: In-memory caching prevents model reloading.  
  â””â”€ **NEW**: Cache management with clear_cache method.  
  â””â”€ **NEW**: Custom cache directory for Hugging Face models.  

## ðŸŸª Error Handling and Fallback Flow

Error occurs during any operation  
â†“  
[Logger.log_to_sublog, core/logger.py](https://github.com/anilbattini/codebase-qa/blob/main/core/logger.py)  
  â””â”€ **ENHANCED**: Comprehensive error logging with context.  
  â””â”€ **NEW**: Provider-specific error logging.  
  â””â”€ **NEW**: Cache status and model loading error logging.  
â†“  
[Error handling in core functions, various files](https://github.com/anilbattini/codebase-qa/blob/main/core/)  
  â””â”€ **ENHANCED**: Graceful error handling with user feedback.  
  â””â”€ **NEW**: Provider-specific error handling and fallbacks.  
  â””â”€ **NEW**: Cache corruption handling and recovery.  
â†“  
[Fallback mechanisms, various files](https://github.com/anilbattini/codebase-qa/blob/main/core/)  
  â””â”€ **NEW**: Fallback to Ollama if Hugging Face fails.  
  â””â”€ **NEW**: Fallback to direct LLM calls if QA chain fails.  
  â””â”€ **NEW**: Fallback to content-hash tracking if Git tracking fails.  
â†“  
[User notification, core/app.py](https://github.com/anilbattini/codebase-qa/blob/main/core/app.py)  
  â””â”€ **ENHANCED**: Clear error messages with actionable information.  
  â””â”€ **NEW**: Provider-specific error messages and solutions.  
  â””â”€ **NEW**: Cache status and model availability information.  

## ðŸ”„ Key Improvements Summary

### **Multi-Provider Model System**
- **Abstract Interface**: ModelProvider ABC for consistent provider interface
- **Provider Implementations**: OllamaProvider and HuggingFaceProvider
- **Factory Pattern**: ModelProviderFactory for creating provider instances
- **Global Management**: Centralized provider switching and management

### **Model Caching & Performance**
- **In-Memory Caching**: Prevents model reloading on every access
- **Lazy Loading**: Models loaded only when needed
- **Cache Management**: Automatic cache clearing during provider switching
- **Custom Cache Directory**: Dedicated Hugging Face cache location

### **Enhanced RAG Management**
- **Lazy Loading**: Retriever and QA chain loaded only when needed
- **Incremental Builds**: Smart rebuild detection and processing
- **Git Tracking**: Enhanced change detection with commit differences
- **User Experience**: Clear progress indicators and confirmation flows

### **Improved Error Handling**
- **Provider-Specific Errors**: Tailored error messages and solutions
- **Graceful Fallbacks**: Multiple fallback mechanisms for robustness
- **Comprehensive Logging**: Detailed logging for debugging and monitoring
- **User Feedback**: Clear error messages with actionable information

This enhanced functional flow provides a comprehensive understanding of the new multi-provider system, model caching, and incremental build capabilities while maintaining backward compatibility with existing Ollama functionality.
