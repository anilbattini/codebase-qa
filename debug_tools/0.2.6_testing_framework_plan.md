# RAG Codebase QA Tool â€“ v0.2.6  
### Comprehensive Automated Questionnaire Testing Plan ğŸ“‹

***

## 1  Purpose  

Create a **fully-automated, source-driven test suite** that:

1. Runs every question in the five-level developer questionnaire through the live RAG system (`chat_handler.process_query`).  
2. Records requests + responses to `codebase-qa_<project_type>/logs/queries/` for perfect traceability.  
3. Injects synthetic (or optional real) user feedbackâ€”rating (1-5â˜…), like/dislike, free-text remarks.  
4. Analyses accuracy, latency & satisfaction against strict per-level targets.  
5. Generates machine-readable JSON and human-readable Markdown reports for CI/CD gating and root-cause debugging.

***

## 2  Guiding Principles  

-  **Real integration** â€“ no mocks; every test path uses production code.  
-  **Headless & CI-ready** â€“ zero Streamlit/session dependencies.  
-  **Project-aware** â€“ auto-detect Android / iOS / Web / Python etc. and apply language-specific checks.  
-  **Live artefact validation** â€“ assertions read the same logs the UI shows.  
-  **World-class structure** â€“ clear separation of reusable framework vs. executable suites.

***

## 3  Folder Layout  

```
project_root/
â”œâ”€ codebase/                       # Source under analysis
â”œâ”€ codebase_qa/                    # RAG tool
â”‚  â”œâ”€ core/
â”‚  â”‚   â”œâ”€ chat_handler.py
â”‚  â”‚   â”œâ”€ context_builder.py
â”‚  â”‚   â”œâ”€ metadata_extractor.py
â”‚  â”‚   â”œâ”€ rag_manager.py
â”‚  â”‚   â”œâ”€ model_config.py
â”‚  â”‚   â”œâ”€ prompt_router.py
â”‚  â”‚   â””â”€ â€¦ (existing modules)
â”‚  â”œâ”€ test_framework/              # Reusable plumbing
â”‚  â”‚   â”œâ”€ base_test.py
â”‚  â”‚   â”œâ”€ questionnaire_driver.py
â”‚  â”‚   â”œâ”€ feedback_collector.py
â”‚  â”‚   â”œâ”€ response_analyzer.py
â”‚  â”‚   â””â”€ logger.py
â”‚  â””â”€ test_suites/                 # Concrete suites
â”‚      â””â”€ questionnaire_full_test.py
â””â”€ codebase_qa_<project_type>/     # Generated data
   â””â”€ logs/
       â”œâ”€ queries/                 # One JSON per Q&A
       â”œâ”€ chat_handler.log
       â””â”€ retrieval.log
```

***

## 4  End-to-End Workflow  

```
Detect project âœ Parse questionnaire âœ
for each question:
    call chat_handler.process_query
    save response JSON to logs/queries/
    inject feedback (rating + remarks)
After loop:
    analyse logs + feedback
    compute metrics & generate reports
```

***

## 5  Validation Targets  

| Level | Capability                              | Accuracy | 95-th Latency |
|-------|-----------------------------------------|----------|---------------|
| 1     | Basic metadata & structure              |  â‰¥95%   | â‰¤ 2 s |
| 2     | Code element location & usage           |  â‰¥85%   | â‰¤ 3 s |
| 3     | Relationships & call-flows              |  â‰¥75%   | â‰¤ 5 s |
| 4     | Semantic understanding & reasoning      |  â‰¥65%   | â‰¤ 8 s |
| 5     | Deep architecture & cross-module debug  |  â‰¥50%   | â‰¤ 12 s |

A response rated â˜… â˜… â˜… â˜… or â˜… â˜… â˜… â˜… â˜… is considered â€œsatisfiedâ€.

***

## 6  Core Framework Modules (to be coded once plan is approved)  

1. **base_test.py** â€“ bootstraps RAG, provides timing/log utilities.  
2. **questionnaire_driver.py** â€“ parses `codebase_qa_questionnaire.md` â†’ `{level: [questions]}`.  
3. **feedback_collector.py** â€“ heuristic scoring âœ rating/like/remarks JSON.  
4. **response_analyzer.py** â€“ cross-checks answers vs. golden data, aggregates KPIs.  
5. **logger.py** â€“ structured JSON logging reused by all tests.  
6. **test_suites/questionnaire_full_test.py** â€“ orchestrates full run; CLI-friendly for CI.

***

## 7  Artefacts Produced per Run  

-  `questionnaire_results_<timestamp>.json` â€“ every Q&A with metadata  
-  `questionnaire_feedback_<timestamp>.json` â€“ ratings & comments  
-  `questionnaire_report_<timestamp>.md` â€“ summary for humans  
-  Standard `.log` files for deeper trace

***

## 8  Execution Example  

```bash
# After building the RAG index
cd codebase-qa
python -m test_suites.questionnaire_full_test \
       --project_dir ../my_project \
       --questionnaire codebase_qa_questionnaire.md \
       --mode full
```

***

## 9  Next Steps  

1. **Confirm this document**.  
2. Implement framework modules (sections 6.1-6.6).  
3. Wire into CI/CD; block merges that lower satisfaction or exceed latency budgets.  
4. Iterate: refine scoring heuristics, extend golden-answer sets, add component-level suites.

***
