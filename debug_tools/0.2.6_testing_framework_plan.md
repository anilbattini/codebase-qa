# RAG Codebase QA Tool – v0.2.6  
### Comprehensive Automated Questionnaire Testing Plan 📋

***

## 1  Purpose  

Create a **fully-automated, source-driven test suite** that:

1. Runs every question in the five-level developer questionnaire through the live RAG system (`chat_handler.process_query`).  
2. Records requests + responses to `codebase-qa_<project_type>/logs/queries/` for perfect traceability.  
3. Injects synthetic (or optional real) user feedback—rating (1-5★), like/dislike, free-text remarks.  
4. Analyses accuracy, latency & satisfaction against strict per-level targets.  
5. Generates machine-readable JSON and human-readable Markdown reports for CI/CD gating and root-cause debugging.

***

## 2  Guiding Principles  

-  **Real integration** – no mocks; every test path uses production code.  
-  **Headless & CI-ready** – zero Streamlit/session dependencies.  
-  **Project-aware** – auto-detect Android / iOS / Web / Python etc. and apply language-specific checks.  
-  **Live artefact validation** – assertions read the same logs the UI shows.  
-  **World-class structure** – clear separation of reusable framework vs. executable suites.

***

## 3  Folder Layout  

```
project_root/
├─ codebase/                       # Source under analysis
├─ codebase_qa/                    # RAG tool
│  ├─ core/
│  │   ├─ chat_handler.py
│  │   ├─ context_builder.py
│  │   ├─ metadata_extractor.py
│  │   ├─ rag_manager.py
│  │   ├─ model_config.py
│  │   ├─ prompt_router.py
│  │   └─ … (existing modules)
│  ├─ test_framework/              # Reusable plumbing
│  │   ├─ base_test.py
│  │   ├─ questionnaire_driver.py
│  │   ├─ feedback_collector.py
│  │   ├─ response_analyzer.py
│  │   └─ logger.py
│  └─ test_suites/                 # Concrete suites
│      └─ questionnaire_full_test.py
└─ codebase_qa_<project_type>/     # Generated data
   └─ logs/
       ├─ queries/                 # One JSON per Q&A
       ├─ chat_handler.log
       └─ retrieval.log
```

***

## 4  End-to-End Workflow  

```
Detect project ➜ Parse questionnaire ➜
for each question:
    call chat_handler.process_query
    save response JSON to logs/queries/
    inject feedback (rating + remarks)
After loop:
    analyse logs + feedback
    compute metrics & generate reports
```

***

## 5  Validation Targets  

| Level | Capability                              | Accuracy | 95-th Latency |
|-------|-----------------------------------------|----------|---------------|
| 1     | Basic metadata & structure              |  ≥95%   | ≤ 2 s |
| 2     | Code element location & usage           |  ≥85%   | ≤ 3 s |
| 3     | Relationships & call-flows              |  ≥75%   | ≤ 5 s |
| 4     | Semantic understanding & reasoning      |  ≥65%   | ≤ 8 s |
| 5     | Deep architecture & cross-module debug  |  ≥50%   | ≤ 12 s |

A response rated ★ ★ ★ ★ or ★ ★ ★ ★ ★ is considered “satisfied”.

***

## 6  Core Framework Modules (to be coded once plan is approved)  

1. **base_test.py** – bootstraps RAG, provides timing/log utilities.  
2. **questionnaire_driver.py** – parses `codebase_qa_questionnaire.md` → `{level: [questions]}`.  
3. **feedback_collector.py** – heuristic scoring ➜ rating/like/remarks JSON.  
4. **response_analyzer.py** – cross-checks answers vs. golden data, aggregates KPIs.  
5. **logger.py** – structured JSON logging reused by all tests.  
6. **test_suites/questionnaire_full_test.py** – orchestrates full run; CLI-friendly for CI.

***

## 7  Artefacts Produced per Run  

-  `questionnaire_results_<timestamp>.json` – every Q&A with metadata  
-  `questionnaire_feedback_<timestamp>.json` – ratings & comments  
-  `questionnaire_report_<timestamp>.md` – summary for humans  
-  Standard `.log` files for deeper trace

***

## 8  Execution Example  

```bash
# After building the RAG index
cd codebase-qa
python -m test_suites.questionnaire_full_test \
       --project_dir ../my_project \
       --questionnaire codebase_qa_questionnaire.md \
       --mode full
```

***

## 9  Next Steps  

1. **Confirm this document**.  
2. Implement framework modules (sections 6.1-6.6).  
3. Wire into CI/CD; block merges that lower satisfaction or exceed latency budgets.  
4. Iterate: refine scoring heuristics, extend golden-answer sets, add component-level suites.

***
